{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7u6GFFxYr5ETZW3cXLFIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARDIK218/Large-Language-Model/blob/main/Building_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## https://www.youtube.com/watch?v=v1tyQtncsE4&t=15s"
      ],
      "metadata": {
        "id": "Oj_E9qacxlKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXZWt6G_s1bD",
        "outputId": "677825c7-aaea-442f-8c5d-58ff922b7f8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/225.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qg-q2fBJxowg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipqt-A2btI2P",
        "outputId": "be1adaa8-1ec9-4cfe-e25a-1ec99006c3a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ii8VTs8Qr1cy"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from IPython.display import Markdown\n",
        "client = OpenAI(api_key=\"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\")\n",
        "\n",
        "def get_response(prompt_question):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "output = get_response(\"Create a simple task list of 3 desktop things I can do on the terminal.\")\n",
        "Markdown(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "nWB08y3Ks1cP",
        "outputId": "55b8c757-9285-4fc6-bdb6-935fa8cde967"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. View directory contents: Use the command `ls` to list the files and folders in the current directory.\n2. Create a new directory: Use the command `mkdir <directory_name>` to create a new directory with the specified name.\n3. Rename a file or folder: Use the command `mv <current_name> <new_name>` to rename a file or folder. Replace `<current_name>` with the current name of the file or folder, and `<new_name>` with the desired new name."
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWithTools:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def get_response(self, prompt_question):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt_question}]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    def execute_function_call(self, function_call_string: str):\n",
        "        exec(function_call_string)\n",
        "\n",
        "\n",
        "model = ModelWithTools(\"gpt-3.5-turbo-16k\")\n",
        "task_description = \"Create a folder called 'lucas-the-unoriginal-joker'.\"\n",
        "output = model.get_response(f\"\"\"Given a task that will be fed as input, and consider you have access to the following functions:\n",
        "\n",
        "    def create_directory(self, directory_name):\n",
        "        '''Function that creates a directory given a directory name.'''\n",
        "        subprocess.run([\"mkdir\", directory_name])\n",
        "\n",
        "    def create_file(self, file_name):\n",
        "        '''Function that creates a file given a file name.'''\n",
        "        subprocess.run([\"touch\", file_name])\n",
        "\n",
        "    def list_files(self):\n",
        "       '''Function that lists all files in the current directory.'''\n",
        "        subprocess.run([\"ls\"])\n",
        "\n",
        "    Your output should be the first function to be executed to complete the task containing the necessary arguments.\n",
        "    For example:\n",
        "\n",
        "    task: 'create a folder named lucas-the-agent-master'\n",
        "    output: model.create_directory('lucas-the-agent-master')\n",
        "\n",
        "    task: 'create a file named the-10-master-rules.md'\n",
        "    output: model.create_file('the-10-master-rules.md')\n",
        "\n",
        "    The OUTPUT SHOULD ONLY BE THE PYTHON FUNCTION CALL and NOTHING ELSE.\n",
        "    task: {task_description}\n",
        "    output:\\n\n",
        "    \"\"\")\n",
        "\n",
        "Markdown(output)"
      ],
      "metadata": {
        "id": "FVwS-dTOtrEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scaling this process using openAI functions\n",
        "\n",
        "\n",
        "Ok, let's first understand how OpenAI the company behind ChatGPT, allows for these function call implementations in its API.\n",
        "\n",
        "OpenAI implemented a function calling API which is a standard way to connect their models to outside tools like in the very simple example we did above.\n",
        "\n",
        "According to their official documentation the sequence of steps for function calling is as follows:\n",
        "\n",
        "Call the model with the user query and a set of functions defined in the functions parameter.\n",
        "The model can choose to call one or more functions; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may hallucinate parameters).\n",
        "Parse the string into JSON in your code, and call your function with the provided arguments if they exist.\n",
        "Call the model again by appending the function response as a new message, and let the model summarize the results back to the user."
      ],
      "metadata": {
        "id": "fRXh3-G3yovk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n",
        "    elif \"paris\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
        "\n",
        "def run_conversation():\n",
        "    # Step 1: send the conversation and available functions to the model\n",
        "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
        "    tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_current_weather\",\n",
        "                \"description\": \"Get the current weather in a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                        },\n",
        "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "    # Step 2: check if the model wanted to call a function\n",
        "    if tool_calls:\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"get_current_weather\": get_current_weather,\n",
        "        }  # only one function in this example, but you can have multiple\n",
        "        messages.append(response_message)  # extend conversation with assistant's reply\n",
        "        # Step 4: send the info for each function call and function response to the model\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(\n",
        "                location=function_args.get(\"location\"),\n",
        "                unit=function_args.get(\"unit\"),\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )  # extend conversation with function response\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=messages,\n",
        "        )  # get a new response from the model where it can see the function response\n",
        "        return second_response\n",
        "output = run_conversation()\n",
        "output"
      ],
      "metadata": {
        "id": "G8gHigWlxvlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on my own function creating tools"
      ],
      "metadata": {
        "id": "xU7OxQjb3Gcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def create_directory(directory_name):\n",
        "    \"\"\"Function that creates a directory given a directory name.\"\"\"\"\"\n",
        "    subprocess.run([\"mkdir\", directory_name])\n",
        "    return json.dumps({\"directory_name\": directory_name})\n",
        "\n",
        "\n",
        "tool_create_directory = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"create_directory\",\n",
        "        \"description\": \"Create a directory given a directory name.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"directory_name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The name of the directory to create.\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"directory_name\"],\n",
        "        },\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "6VBnuUmZ1CzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def run_terminal_task():\n",
        "    messages = [{\"role\": \"user\", \"content\": \"Create a folder called 'lucas-the-super-unoriginal-joker'.\"}]\n",
        "    tools = [tool_create_directory]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "    # Step 2: check if the model wanted to call a function\n",
        "\n",
        "    if tool_calls:\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"create_directory\": create_directory,\n",
        "        }\n",
        "        messages.append(response_message)\n",
        "        # Step 4: send the info for each function call and function response to the model\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(\n",
        "                directory_name=function_args.get(\"directory_name\"),\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "        )\n",
        "        return second_response\n",
        "\n",
        "output = run_terminal_task()\n",
        "output"
      ],
      "metadata": {
        "id": "CeFXZhRiYdXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "def calculate_wasted_cost(gpu_units, motherboard_units, cpu_units, gpu_cost_per_unit, motherboard_cost_per_unit, cpu_cost_per_unit):\n",
        "    \"\"\"Function that calculates the cost of wasted GPU, motherboard, and CPU.\"\"\"\n",
        "    total_gpu_cost = gpu_units * gpu_cost_per_unit\n",
        "    total_motherboard_cost = motherboard_units * motherboard_cost_per_unit\n",
        "    total_cpu_cost = cpu_units * cpu_cost_per_unit\n",
        "\n",
        "    wasted_cost = total_gpu_cost + total_motherboard_cost + total_cpu_cost\n",
        "\n",
        "    return json.dumps({\n",
        "        \"gpu_units\": gpu_units,\n",
        "        \"motherboard_units\": motherboard_units,\n",
        "        \"cpu_units\": cpu_units,\n",
        "        \"gpu_cost_per_unit\": gpu_cost_per_unit,\n",
        "        \"motherboard_cost_per_unit\": motherboard_cost_per_unit,\n",
        "        \"cpu_cost_per_unit\": cpu_cost_per_unit,\n",
        "        \"wasted_cost\": wasted_cost\n",
        "    })\n",
        "\n",
        "tool_calculate_wasted_cost = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"calculate_wasted_cost\",\n",
        "        \"description\": \"Calculate the cost of wasted GPU, motherboard, and CPU.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"gpu_units\": {\"type\": \"integer\", \"description\": \"Number of wasted GPU units.\"},\n",
        "                \"motherboard_units\": {\"type\": \"integer\", \"description\": \"Number of wasted motherboard units.\"},\n",
        "                \"cpu_units\": {\"type\": \"integer\", \"description\": \"Number of wasted CPU units.\"},\n",
        "                \"gpu_cost_per_unit\": {\"type\": \"number\", \"description\": \"Cost per unit of GPU.\"},\n",
        "                \"motherboard_cost_per_unit\": {\"type\": \"number\", \"description\": \"Cost per unit of motherboard.\"},\n",
        "                \"cpu_cost_per_unit\": {\"type\": \"number\", \"description\": \"Cost per unit of CPU.\"}\n",
        "            },\n",
        "            \"required\": [\"gpu_units\", \"motherboard_units\", \"cpu_units\", \"gpu_cost_per_unit\", \"motherboard_cost_per_unit\", \"cpu_cost_per_unit\"]\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# Example usage:\n",
        "# calculate_wasted_cost(5, 2, 3, 200, 600, 700)\n"
      ],
      "metadata": {
        "id": "-ahbZ7fLZZeb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def run_terminal_task():\n",
        "    # Initial user command\n",
        "    user_command = {\"role\": \"user\", \"content\": \"Calculate the wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs.\"}\n",
        "\n",
        "    # Tools available for the conversation\n",
        "    tools = [tool_calculate_wasted_cost]\n",
        "\n",
        "    # Messages exchanged in the conversation\n",
        "    messages = [user_command]\n",
        "\n",
        "    # Step 1: Request completion from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "\n",
        "    # Step 2: Retrieve the response and tool calls\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    # Step 3: Check if the model wanted to call a function\n",
        "    if tool_calls:\n",
        "        # Step 4: Call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"calculate_wasted_cost\": calculate_wasted_cost,\n",
        "        }\n",
        "        messages.append(response_message)\n",
        "\n",
        "        # Step 5: Send the info for each function call and function response to the model\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(\n",
        "                gpu_units=function_args.get(\"gpu_units\"),\n",
        "                motherboard_units=function_args.get(\"motherboard_units\"),\n",
        "                cpu_units=function_args.get(\"cpu_units\"),\n",
        "                gpu_cost_per_unit=function_args.get(\"gpu_cost_per_unit\"),\n",
        "                motherboard_cost_per_unit=function_args.get(\"motherboard_cost_per_unit\"),\n",
        "                cpu_cost_per_unit=function_args.get(\"cpu_cost_per_unit\"),\n",
        "            )\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": function_response,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Step 6: Get the second response from the model\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "        )\n",
        "\n",
        "        return second_response\n",
        "\n",
        "# Example usage\n",
        "output = run_terminal_task()\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FRV02GiZvVX",
        "outputId": "11d40518-078f-4833-eee6-0ab3eb05a6e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8dd7IGnQivMvAug550p1lX00ZX1jE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs is $3800.', role='assistant', function_call=None, tool_calls=None))], created=1704455848, model='gpt-3.5-turbo-16k-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=161, total_tokens=183))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WkuDwDliacAh",
        "outputId": "36191252-3107-4c61-a043-0d5a0770e66e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs is $3800.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LangChain Agents"
      ],
      "metadata": {
        "id": "2TXj0sFmbJ4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kutqq9WEbVT8",
        "outputId": "753780c9-e2a5-454d-88e8-4baae82b8bda"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.354-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.8 (from langchain)\n",
            "  Downloading langchain_community-0.0.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.5 (from langchain)\n",
            "  Downloading langchain_core-0.1.6-py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)\n",
            "  Downloading langsmith-0.0.77-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.354 langchain-community-0.0.8 langchain-core-0.1.6 langsmith-0.0.77 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
        "\n",
        "\n",
        "@tool\n",
        "def calculate_wasted_cost(gpu_units, motherboard_units, cpu_units, gpu_cost_per_unit=500, motherboard_cost_per_unit=700, cpu_cost_per_unit=400):\n",
        "    \"\"\"Calculates the total wasted cost of GPUs, motherboards, and CPUs.\n",
        "\n",
        "    Args:\n",
        "        gpu_units: The number of GPUs to calculate the cost for.\n",
        "        motherboard_units: The number of motherboards to calculate the cost for.\n",
        "        cpu_units: The number of CPUs to calculate the cost for.\n",
        "        gpu_cost_per_unit: The cost of a single GPU (default: 500).\n",
        "        motherboard_cost_per_unit: The cost of a single motherboard (default: 700).\n",
        "        cpu_cost_per_unit: The cost of a single CPU (default: 400).\n",
        "\n",
        "    Returns:\n",
        "        A JSON object containing the total wasted cost.\n",
        "    \"\"\"\n",
        "\n",
        "    total_gpu_cost = gpu_units * gpu_cost_per_unit\n",
        "    total_motherboard_cost = motherboard_units * motherboard_cost_per_unit\n",
        "    total_cpu_cost = cpu_units * cpu_cost_per_unit\n",
        "    wasted_cost = total_gpu_cost + total_motherboard_cost + total_cpu_cost\n",
        "    return json.dumps({\n",
        "        \"gpu_units\": gpu_units,\n",
        "        \"motherboard_units\": motherboard_units,\n",
        "        \"cpu_units\": cpu_units,\n",
        "        \"gpu_cost_per_unit\": gpu_cost_per_unit,\n",
        "        \"motherboard_cost_per_unit\": motherboard_cost_per_unit,\n",
        "        \"cpu_cost_per_unit\": cpu_cost_per_unit,\n",
        "        \"wasted_cost\": wasted_cost\n",
        "    })\n",
        "\n",
        "\n",
        "tools = [calculate_wasted_cost]\n",
        "\n",
        "\n",
        "llm_chat = ChatOpenAI(api_key=\"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\", temperature=0)\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "[\n",
        "    (\"system\", \"You are a very powerful assistant that helps\\\n",
        "                users perform tasks in the terminal.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "llm_with_tools = llm_chat.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
        "\n",
        "agent = (\n",
        "{\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
        "        x[\"intermediate_steps\"]\n",
        "    ),\n",
        "}\n",
        "| prompt\n",
        "| llm_with_tools\n",
        "| OpenAIFunctionsAgentOutputParser())\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "action_input = \"Calculate the wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs\"\n",
        "\n",
        "# Invoke the agent with the action input\n",
        "output = agent_executor.invoke({\"input\": action_input})\n",
        "\n",
        "# Print the output\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdPary-tbPIE",
        "outputId": "36b3fd41-9161-429a-caab-8cdbad2f7a94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `calculate_wasted_cost` with `{'gpu_units': 5, 'motherboard_units': 2, 'cpu_units': 3}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{\"gpu_units\": 5, \"motherboard_units\": 2, \"cpu_units\": 3, \"gpu_cost_per_unit\": 500, \"motherboard_cost_per_unit\": 700, \"cpu_cost_per_unit\": 400, \"wasted_cost\": 5100}\u001b[0m\u001b[32;1m\u001b[1;3mThe total wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs is $5100.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'Calculate the wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs', 'output': 'The total wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs is $5100.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VqDynd_c92k",
        "outputId": "2624444d-a0dc-42bd-9f6a-5baba374da30"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total wasted cost for 5 GPUs, 2 motherboards, and 3 CPUs is $5100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpHAc_dkf07Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}