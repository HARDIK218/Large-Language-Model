{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbhvURDBbHAmJbiEkRm1C3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARDIK218/Large-Language-Model/blob/main/LLM_pdf_with_Astradb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNnUud6m1YuV",
        "outputId": "1d3064ff-df1a-46d9-8aff-87b38bb10f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m785.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#langchain components used\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "#support for dataset retrival with Hugging Face\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "import cassio"
      ],
      "metadata": {
        "id": "AaVIAsAt3kL-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcvEHSHE5Tiw",
        "outputId": "db9a5ac5-40d9-4e6d-b11f-79727b2a444e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/232.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "5mTVFYnl-ru6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = \"AstraCS:QvfOsyZBHAKubbvlJNepvFtc:07a95a0f9213cfd0e36501214bb64c3e7d31cc9756810dd6f5dd1e613eb44c7c\"\n",
        "ASTRA_DB_ID = \"87e9662f-19ab-4535-8a1d-63d259ba52a8\"\n",
        "\n",
        "OPENAI_API_KEY = \"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\""
      ],
      "metadata": {
        "id": "6ldZtFRK_H1i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfreader = PdfReader('/content/classification project.pdf')"
      ],
      "metadata": {
        "id": "0G53GNQwGCBj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "raw_text = ''\n",
        "for i,page in enumerate(pdfreader.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    raw_text += content"
      ],
      "metadata": {
        "id": "ELoZSqZoGCFN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the connection to your database\n",
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id = ASTRA_DB_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yfOm0eUGyxn",
        "outputId": "9a3c79b2-04e0-4c95-c73d-0741a6cc6dd9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 87e9662f-19ab-4535-8a1d-63d259ba52a8-us-east1.db.astra.datastax.com:29042:33eb8141-aa77-405a-ac6e-c24712cf48b9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 87e9662f-19ab-4535-8a1d-63d259ba52a8-us-east1.db.astra.datastax.com:29042:33eb8141-aa77-405a-ac6e-c24712cf48b9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(134857021710224) 87e9662f-19ab-4535-8a1d-63d259ba52a8-us-east1.db.astra.datastax.com:29042:33eb8141-aa77-405a-ac6e-c24712cf48b9> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 87e9662f-19ab-4535-8a1d-63d259ba52a8-us-east1.db.astra.datastax.com:29042:33eb8141-aa77-405a-ac6e-c24712cf48b9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create the Langchain embedding and llm objects fro later usage\n",
        "\n",
        "llm = OpenAI(openai_api_key = OPENAI_API_KEY)\n",
        "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "m1piH-3THqLV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create your LangChain vector store\n",
        "astra_vector_store = Cassandra(\n",
        "    embedding = embeddings,\n",
        "    table_name = \"qa_mini_demo\",\n",
        "    session = None,\n",
        "    keyspace=None,\n",
        ")"
      ],
      "metadata": {
        "id": "BB2J8XWYHqM8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "yEIOXAZ4HqQF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUmSc0bKHqTd",
        "outputId": "5bc3c41b-2a6c-432c-f9e4-b77addf45baa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Contents\\n1 Introduction 4\\n1.1 Goals of the Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Paper Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Literature Review 4\\n2.1 Stock Market Prediction Using Bayesian-Regularized Neural Networks . . . . . . . . 4\\n2.2 Stock Market Prediction Using A Machine Learning Model . . . . . . . . . . . . . . . 5\\n2.3 House Price Prediction Using Multilevel Model and Neural Networks . . . . . . . . 6\\n2.4 Composition of Models and Feature Engineering to Win Algorithmic Trading Chal-\\nlenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.5 Using K-Nearest Neighbours for Stock Price Prediction . . . . . . . . . . . . . . . . . 8',\n",
              " 'lenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.5 Using K-Nearest Neighbours for Stock Price Prediction . . . . . . . . . . . . . . . . . 8\\n3 Data Preparation 10\\n3.1 Data Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.2 Reading the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.3 Getting A Feel of the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.4 Data Cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.4.1 Dealing with Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18',\n",
              " '3.4 Data Cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.4.1 Dealing with Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n3.5 Outlier Removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n3.6 Deleting Some Unimportant Columns . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4 Exploratory Data Analysis 27\\n4.1 Target Variable Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.2 Correlation Between Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n4.2.1 Relatioships Between the Target Variable and Other Varibles . . . . . . . . . . 30',\n",
              " '4.2 Correlation Between Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n4.2.1 Relatioships Between the Target Variable and Other Varibles . . . . . . . . . . 30\\n4.2.2 Relatioships Between Predictor Variables . . . . . . . . . . . . . . . . . . . . . 37\\n4.3 Feature Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n4.3.1 Creating New Derived Features . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n4.3.2 Dealing with Ordinal Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n4.3.3 One-Hot Encoding For Categorical Features . . . . . . . . . . . . . . . . . . . 42\\n5 Prediction Type and Modeling Techniques 43\\n5.0.1 1. Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43',\n",
              " '5 Prediction Type and Modeling Techniques 43\\n5.0.1 1. Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n5.0.2 2. Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.0.3 3. Support Vector Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.0.4 4. Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.0.5 5. Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.0.6 6. Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n5.0.7 7. Gradient Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n6 Model Building and Evaluation 45',\n",
              " '5.0.7 7. Gradient Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n6 Model Building and Evaluation 45\\n6.1 Feature Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n6.2 Splitting the Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n6.3 Modeling Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n6.3.1 Searching for Effective Parameters . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n6.4 Performance Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\n6.5 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48',\n",
              " '6.5 Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n26.5.1 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n6.5.2 Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n6.5.3 Support Vector Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n6.5.4 Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\\n6.5.5 Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n6.5.6 Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n6.5.7 Gradient Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56',\n",
              " '6.5.6 Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n6.5.7 Gradient Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n7 Analysis and Comparison 57\\n7.1 Performance Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n7.2 Feature Importances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n7.2.1 XGBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n7.2.2 Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n7.2.3 Common Important Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n8 Conclusion 65\\n9 References 65\\n31 Introduction',\n",
              " '7.2.3 Common Important Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n8 Conclusion 65\\n9 References 65\\n31 Introduction\\nThousands of houses are sold everyday. There are some questions every buyer asks himself like:\\nWhat is the actual price that this house deserves? Am I paying a fair price? In this paper, a machine\\nlearning model is proposed to predict a house price based on data related to the house (its size,\\nthe year it was built in, etc.). During the development and evaluation of our model, we will show\\nthe code used for each step followed by its output. This will facilitate the reproducibility of our\\nwork. In this study, Python programming language with a number of Python packages will be\\nused.\\n1.1 Goals of the Study\\nThe main objectives of this study are as follows:',\n",
              " 'work. In this study, Python programming language with a number of Python packages will be\\nused.\\n1.1 Goals of the Study\\nThe main objectives of this study are as follows:\\n•To apply data preprocessing and preparation techniques in order to obtain clean data\\n•To build machine learning models able to predict house price based on house features\\n•To analyze and compare models performance in order to choose the best model\\n1.2 Paper Organization\\nThis paper is organized as follows: in the next section, section 2, we examine studies related to\\nour work from scientiﬁc journals. In section 3, we go through data preparation including data\\ncleaning, outlier removal, and feature engineering. Next in section 4, we discuss the type of our',\n",
              " 'our work from scientiﬁc journals. In section 3, we go through data preparation including data\\ncleaning, outlier removal, and feature engineering. Next in section 4, we discuss the type of our\\nproblem and the type of machine-learning prediction that should be applied; we also list the pre-\\ndiction techniques that will be used. In section 5, we choose algorithms to implement the tech-\\nniques in section 4; we build models based on these algorithms; we also train and test each model.\\nIn section 6, we analyze and compare the results we got from section 5 and conclude the paper.\\n2 Literature Review\\nIn this section, we look at ﬁve recent studies that are related to our topic and see how models were\\nbuilt and what results were achieved in these studies.',\n",
              " '2 Literature Review\\nIn this section, we look at ﬁve recent studies that are related to our topic and see how models were\\nbuilt and what results were achieved in these studies.\\n2.1 Stock Market Prediction Using Bayesian-Regularized Neural Networks\\nIn a study done by Ticknor (2013), he used Bayesian regularized articial neural network to predict\\nthe future operation of ﬁnancial market. Speciﬁcally, he built a model to predict future stock\\nprices. The input of the model is previous stock statistics in addition to some ﬁnancial technical\\ndata. The output of the model is the next-day closing price of the corresponding stocks.\\nThe model proposed in the study is built using Bayesian regularized neural network. The',\n",
              " 'data. The output of the model is the next-day closing price of the corresponding stocks.\\nThe model proposed in the study is built using Bayesian regularized neural network. The\\nweights of this type of networks are given a probabilistic nature. This allows the network to\\npenalize very complex models (with many hidden layers) in an automatic manner. This in turn\\nwill reduce the overﬁtting of the model.\\nThe model consists of a feedforward neural network which has three layers: an input layer, one\\nhidden layer, and an output layer. The author chose the number of neurons in the hidden layer\\nbased on experimental methods.The input data of the model is normalized to be between -1and\\n1, and this opertion is reversed for the output so the predicted price appears in the appropriate\\nscale.',\n",
              " 'based on experimental methods.The input data of the model is normalized to be between -1and\\n1, and this opertion is reversed for the output so the predicted price appears in the appropriate\\nscale.\\n4Figure 1: Predicted vs. actual price\\nThe data that was used in this study was obtained from Goldman Sachs Group (GS), Inc. and\\nMicrosoft Corp. (MSFT) . The data covers 734 trading days (4 January 2010 to 31 December 2012).\\nEach instance of the data consisted of daily statistics: low price, high price, opening price, close\\nprice, and trading volume. To facilitate the training and testing of the model, this data was split\\ninto training data and test data with 80%and 20%of the original data, respectively. In addition',\n",
              " 'price, and trading volume. To facilitate the training and testing of the model, this data was split\\ninto training data and test data with 80%and 20%of the original data, respectively. In addition\\nto the daily-statistics variables in the data, six more variables were created to reﬂect ﬁnancial\\nindicators.\\nThe performance of the model were evaluated using mean absolute percentage error (MAPE)\\nperformance metric. MAPE was calculated using this formula:\\nMAPE =år\\ni=1(abs(yi\\x00pi)/yi)\\nr\\x02100 (1)\\nwhere piis the predicted stock price on day i,yiis the actual stock price on day i, and ris the\\nnumber of trading days.\\nWhen applied on the test data, The model achieved a MAPE score of 1.0561 for MSFT part,\\nand 1.3291 for GS part. Figure 1 shows the actual values and predicted values for both GS and',\n",
              " 'When applied on the test data, The model achieved a MAPE score of 1.0561 for MSFT part,\\nand 1.3291 for GS part. Figure 1 shows the actual values and predicted values for both GS and\\nMSFT data.\\n2.2 Stock Market Prediction Using A Machine Learning Model\\nIn another study done by Hegazy, Soliman, and Salam (2014), a system was proposed to predict\\ndaily stock market prices. The system combines particle swarm optimization (PSO) and least\\nsquare support vector machine (LS-SVM), where PSO was used to optimize LV-SVM.\\nThe authors claim that in most cases, artiﬁcial neural networks (ANNs) are subject to the over-\\nﬁtting problem. They state that support vector machines algorithm (SVM) was developed as an',\n",
              " 'The authors claim that in most cases, artiﬁcial neural networks (ANNs) are subject to the over-\\nﬁtting problem. They state that support vector machines algorithm (SVM) was developed as an\\nalternative that doesn’t suffer from overﬁtting. They attribute this advantage to SVMs being based\\non the solid foundations of VC-theory. They further elaborate that LS-SVM method was refor-\\nmulation of traditional SVM method that uses a regularized least squares function with equality\\n5Figure 2: The structure of the model used\\nconstraints to obtain a linear system that satisﬁes Karush-Kuhn-Tucker conditions for getting an\\noptimal solution.\\nThe authors describe PSO as a popular evolutionary optimization method that was inspired',\n",
              " 'optimal solution.\\nThe authors describe PSO as a popular evolutionary optimization method that was inspired\\nby organism social behavior like bird ﬂocking. They used it to ﬁnd the optimal parameters for LS-\\nSVM. These parameters are the cost penalty C, kernel parameter g, and insensitive loss function\\nϵ.\\nThe model proposed in the study was based on the analysis of historical data and technical\\nﬁnancial indicators and using LS-SVM optimized by PSO to predict future daily stock prices. The\\nmodel input was six vectors representing the historical data and the technical ﬁnancial indicators.\\nThe model output was the future price. The model used is represented in Figure 2.\\nRegarding the technical ﬁnancial indicators, ﬁve were derived from the raw data: relative',\n",
              " 'The model output was the future price. The model used is represented in Figure 2.\\nRegarding the technical ﬁnancial indicators, ﬁve were derived from the raw data: relative\\nstrength index (RSI), money ﬂow index (MFI), exponential moving average (EMA), stochastic os-\\ncillator (SO), and moving average convergence/divergence (MACD). These indicators are known\\nin the domain of stock market.\\nThe model was trained and tested using datasets taken from https://ﬁnance.yahoo.com/. The\\ndatasets were from Jan 2009 to Jan 2012 and include stock data for many companies like Adobe\\nand HP. All datasets were partitioned into a training set with 70%of the data and a test set with\\n30%of the data. Three models were trained and tested: LS-SVM-PSO model, LS-SVM model, and',\n",
              " 'and HP. All datasets were partitioned into a training set with 70%of the data and a test set with\\n30%of the data. Three models were trained and tested: LS-SVM-PSO model, LS-SVM model, and\\nANN model. The results obtained in the study showed that LS-SVM-PSO model had the best\\nperformance. Figure 3 shows a comparison between the mean square error (MSE) of the three\\nmodels for the stocks of many companies.\\n2.3 House Price Prediction Using Multilevel Model and Neural Networks\\nA different study was done by Feng and Jones (2015) to preduct house prices. Two models were\\nbuilt: a multilevel model (MLM) and an artiﬁcial neural network model (ANN). These two models\\nwere compared to each other and to a hedonic price model (HPM).',\n",
              " 'built: a multilevel model (MLM) and an artiﬁcial neural network model (ANN). These two models\\nwere compared to each other and to a hedonic price model (HPM).\\nThe multilevel model integrates the micro-level that speciﬁes the relationships between houses\\nwithin a given neighbourhood, and the macro-level equation which speciﬁes the relationships\\nbetween neighbouhoods. The hedonic price model is a model that estimates house prices using\\n6Figure 3: MSE comparison\\nsome attributes such as the number of bedrooms in the house, the size of the house, etc.\\nThe data used in the study contains house prices in Greater Bristol area between 2001 and 2013.\\nSecondary data was obtained from the Land Registry, the Population Census and Neighbourhood',\n",
              " 'The data used in the study contains house prices in Greater Bristol area between 2001 and 2013.\\nSecondary data was obtained from the Land Registry, the Population Census and Neighbourhood\\nStatistics to be used in order to make the models suitable for national usage. The authors listed\\nmany reasons on why they chose the Greater Bristol area such as its diverse urban and rural blend\\nand its different property types. Each record in the dataset contains data about a house in the area:\\nit contains the address, the unit postcode, property type, the duration (freehold or leasehold), the\\nsale price, the date of the sale, and whether the house was newly-built when it was sold. In total,\\nthe dataset contains around 65,000 entries. To enable model training and testing, the dataset was',\n",
              " 'sale price, the date of the sale, and whether the house was newly-built when it was sold. In total,\\nthe dataset contains around 65,000 entries. To enable model training and testing, the dataset was\\ndivided into a training set that contains data about house sales from 2001 to 2012, and a test set\\nthat contains data about house sales in 2013.\\nThe three models (MLM, ANN, and HPM) were tested using three senarios. In the ﬁrst senario,\\nlocational and measured neighbourhood attributes were not included in the data. In the second\\nsenario, grid references of house location were included in the data. In the third senario, measured\\nneighbourhood attributes were included in the data. The models were compared in goodness of',\n",
              " 'senario, grid references of house location were included in the data. In the third senario, measured\\nneighbourhood attributes were included in the data. The models were compared in goodness of\\nﬁt where R2was the metric, predictive accuracy where mean absolute error (MAE) and mean\\nabsolute percentage error (MAPE) were the metrics, and explanatory power. HPM and MLM\\nmodels were ﬁtted using MLwiN software, and ANN were ﬁtted using IBM SPSS software. Figure\\n4 shows the performance of each model regarding ﬁt goodness and predictive accuracy. It shows\\nthat MLM model has better performance in general than other models.\\n7Figure 4: Model performance comparison\\n2.4 Composition of Models and Feature Engineering to Win Algorithmic Trading\\nChallenge',\n",
              " 'that MLM model has better performance in general than other models.\\n7Figure 4: Model performance comparison\\n2.4 Composition of Models and Feature Engineering to Win Algorithmic Trading\\nChallenge\\nA study done by de Abril and Sugiyama (2013) introduced the techniques and ideas used to win\\nAlgorithmic Trading Challenge, a competition held on Kaggle. The goal of the competition was\\nto develop a model that can predict the short-term response of order-driven markets after a big\\nliquidity shock. A liquidity shock happens when a trade or a sequence of trades causes an acute\\nshortage of liquidity (cash for example).\\nThe challenge data contains a training dataset and a test dataset. The training dataset has\\naround 754,000 records of trade and quote observations for many securities of London Stock',\n",
              " 'The challenge data contains a training dataset and a test dataset. The training dataset has\\naround 754,000 records of trade and quote observations for many securities of London Stock\\nExchange before and after a liquidity shock. A trade event happens when shares are sold or\\nbought, whereas a quote event happens when the ask price or the best bid changes.\\nA separate model was built for bid and another for ask. Each one of these models consists of K\\nrandom-forest sub-models. The models predict the price at a particular future time.\\nThe authors spent much effort on feature engineering. They created more than 150features.\\nThese features belong to four categories: price features, liquidity-book features, spread features',\n",
              " 'The authors spent much effort on feature engineering. They created more than 150features.\\nThese features belong to four categories: price features, liquidity-book features, spread features\\n(bid/ask spread), and rate features (arrival rate of orders/quotes). They applied a feature selection\\nalgorithm to obtain the optimal feature set ( Fb) for bid sub-models and the optimal feature set ( Fa)\\nof all ask sub-models. The algorithm applied eliminates features in a backward manner in order\\nto get a feature set with reasonable computing time and resources.\\nThree instances of the ﬁnal model proposed in the study were trained on three datasets; each\\none of them consists of 50,000 samples sampled randomly from the training dataset. Then, the',\n",
              " 'Three instances of the ﬁnal model proposed in the study were trained on three datasets; each\\none of them consists of 50,000 samples sampled randomly from the training dataset. Then, the\\nthree models were applied to the test dataset. The predictions of the three models were then\\naveraged to obtain the ﬁnal prediction. The proposed method achieved a RMSE score of 0.77\\napproximately.\\n2.5 Using K-Nearest Neighbours for Stock Price Prediction\\nAlkhatib, Najadat, Hmeidi, and Shatnawi (2013) have done a study where they used the k-nearest\\nneighbours (KNN) algorithm to predict stock prices. In this study, they expressed the stock pre-\\ndiction problem as a similarity-based classiﬁcation, and they represented the historical stock data\\nas well as test data by vectors.',\n",
              " 'diction problem as a similarity-based classiﬁcation, and they represented the historical stock data\\nas well as test data by vectors.\\nThe authors listed the steps of predicting the closing price of stock market using KNN as\\nfollows:\\n•The number of neaerest neighbours is chosen\\n8Figure 5: Prediction performance evaluation\\nFigure 6: AIEI lift graph\\n•The distance between the new record and the training data is computed\\n•Training data is sorted according to the calculated distance\\n•Majority voting is applied to the classes of the k nearest neighbours to determine the pre-\\ndicted value of the new record\\nThe data used in the study is stock data of ﬁve companies listed on the Jordanian stock ex-\\nchange. The data range is from 4 June 2009 to 24 December 2009. Each of the ﬁve companies has',\n",
              " 'The data used in the study is stock data of ﬁve companies listed on the Jordanian stock ex-\\nchange. The data range is from 4 June 2009 to 24 December 2009. Each of the ﬁve companies has\\naround 200records in the data. Each record has three variables: closing price, low price, and high\\nprice. The author stated that the closing price is the most important feature in determining the\\nprediction value of a stock using KNN.\\nAfter applying KNN algorithm, the authors summarized the prediction performance evalua-\\ntion using different metrics in a the table shown in Figure 5.\\nThe authors used lift charts also to evaluate the performance of their model. Lift chart shows\\nthe improvement obtained by using the model compared to random estimation. As an example,',\n",
              " 'The authors used lift charts also to evaluate the performance of their model. Lift chart shows\\nthe improvement obtained by using the model compared to random estimation. As an example,\\nthe lift graph for AIEI company is shown in Figure 6. The area between the two lines in the graph\\nis an indicator of the goodness of the model.\\nFigure 7 shows the relationship between the actual price and predicted price for one year for\\nthe same company.\\n9Figure 7: Relationship between actual and predicted price for AIEI\\n3 Data Preparation\\nIn this study, we will use a housing dataset presented by De Cock (2011). This dataset describes\\nthe sales of residential units in Ames, Iowa starting from 2006 until 2010. The dataset contains a',\n",
              " 'In this study, we will use a housing dataset presented by De Cock (2011). This dataset describes\\nthe sales of residential units in Ames, Iowa starting from 2006 until 2010. The dataset contains a\\nlarge number of variables that are involved in determining a house price. We obtained a csv copy\\nof the data from https://www.kaggle.com/prevek18/ames-housing-dataset.\\n3.1 Data Description\\nThe dataset contains 2930 records (rows) and 82features (columns).\\nHere, we will provide a brief description of dataset features. Since the number of features is\\nlarge (82), we will attach the original data description ﬁle to this paper for more information about\\nthe dataset (It can be downloaded also from https://www.kaggle.com/c/house-prices-advanced-',\n",
              " 'large (82), we will attach the original data description ﬁle to this paper for more information about\\nthe dataset (It can be downloaded also from https://www.kaggle.com/c/house-prices-advanced-\\nregression-techniques/data). Now, we will mention the feature name with a short description of\\nits meaning.\\nFeature Description\\nMSSubClass The type of the house involved in the sale\\nMSZoning The general zoning classiﬁcation of the sale\\nLotFrontage Linear feet of street connected to the house\\nLotArea Lot size in square feet\\nStreet Type of road access to the house\\nAlley Type of alley access to the house\\nLotShape General shape of the house\\nLandContour House ﬂatness\\nUtilities Type of utilities available\\nLotConﬁg Lot conﬁguration\\nLandSlope House Slope\\nNeighborhood Locations within Ames city limits',\n",
              " 'LotShape General shape of the house\\nLandContour House ﬂatness\\nUtilities Type of utilities available\\nLotConﬁg Lot conﬁguration\\nLandSlope House Slope\\nNeighborhood Locations within Ames city limits\\n10Feature Description\\nCondition1 Proximity to various conditions\\nCondition2 Proximity to various conditions (if more than one is\\npresent)\\nBldgType House type\\nHouseStyle House style\\nOverallQual Overall quality of material and ﬁnish of the house\\nOverallCond Overall condition of the house\\nYearBuilt Construction year\\nYearRemodAdd Remodel year (if no remodeling nor addition, same as\\nYearBuilt)\\nRoofStyle Roof type\\nRoofMatl Roof material\\nExterior1st Exterior covering on house\\nExterior2nd Exterior covering on house (if more than one material)\\nMasVnrType Type of masonry veneer',\n",
              " 'YearBuilt)\\nRoofStyle Roof type\\nRoofMatl Roof material\\nExterior1st Exterior covering on house\\nExterior2nd Exterior covering on house (if more than one material)\\nMasVnrType Type of masonry veneer\\nMasVnrArea Masonry veneer area in square feet\\nExterQual Quality of the material on the exterior\\nExterCond Condition of the material on the exterior\\nFoundation Foundation type\\nBsmtQual Basement height\\nBsmtCond Basement Condition\\nBsmtExposure Refers to walkout or garden level walls\\nBsmtFinType1 Rating of basement ﬁnished area\\nBsmtFinSF1 Type 1 ﬁnished square feet\\nBsmtFinType2 Rating of basement ﬁnished area (if multiple types)\\nBsmtFinSF2 Type 2 ﬁnished square feet\\nBsmtUnfSF Unﬁnished basement area in square feet\\nTotalBsmtSF Total basement area in square feet\\nHeating Heating type',\n",
              " 'BsmtFinSF2 Type 2 ﬁnished square feet\\nBsmtUnfSF Unﬁnished basement area in square feet\\nTotalBsmtSF Total basement area in square feet\\nHeating Heating type\\nHeatingQC Heating quality and condition\\nCentralAir Central air conditioning\\nElectrical Electrical system type\\n1stFlrSF First ﬂoor area in square feet\\n2ndFlrSF Second ﬂoor area in square feet\\nLowQualFinSF Low quality ﬁnished square feet in all ﬂoors\\nGrLivArea Above-ground living area in square feet\\nBsmtFullBath Basement full bathrooms\\nBsmtHalfBath Basement half bathrooms\\nFullBath Full bathrooms above ground\\nHalfBath Half bathrooms above ground\\nBedroom Bedrooms above ground\\nKitchen Kitchens above ground\\nKitchenQual Kitchen quality\\nTotRmsAbvGrd Total rooms above ground (excluding bathrooms)\\nFunctional Home functionality',\n",
              " 'Bedroom Bedrooms above ground\\nKitchen Kitchens above ground\\nKitchenQual Kitchen quality\\nTotRmsAbvGrd Total rooms above ground (excluding bathrooms)\\nFunctional Home functionality\\nFireplaces Number of ﬁreplaces\\n11Feature Description\\nFireplaceQu Fireplace quality\\nGarageType Garage location\\nGarageYrBlt Year garage was built in\\nGarageFinish Interior ﬁnish of the garage\\nGarageCars Size of garage (in car capacity)\\nGarageArea Garage size in square feet\\nGarageQual Garage quality\\nGarageCond Garage condition\\nPavedDrive How driveway is paved\\nWoodDeckSF Wood deck area in square feet\\nOpenPorchSF Open porch area in square feet\\nEnclosedPorch Enclosed porch area in square feet\\n3SsnPorch Three season porch area in square feet\\nScreenPorch Screen porch area in square feet\\nPoolArea Pool area in square feet',\n",
              " 'EnclosedPorch Enclosed porch area in square feet\\n3SsnPorch Three season porch area in square feet\\nScreenPorch Screen porch area in square feet\\nPoolArea Pool area in square feet\\nPoolQC Pool quality\\nFence Fence quality\\nMiscFeature Miscellaneous feature\\nMiscVal Value of miscellaneous feature\\nMoSold Sale month\\nYrSold Sale year\\nSaleType Sale type\\nSaleCondition Sale condition\\n3.2 Reading the Dataset\\nThe ﬁrst step is reading the dataset from the csv ﬁle we downloaded. We will use the read_csv()\\nfunction from Pandas Python package:\\nimport pandas aspd\\nimport numpy asnp\\ndataset =pd.read_csv( \"AmesHousing.csv \")\\n3.3 Getting A Feel of the Dataset\\nLet’s display the ﬁrst few rows of the dataset to get a feel of it:\\n# Configuring float numbers format\\npd.options .display .float_format =\\'{:20.2f} \\'.format',\n",
              " \"3.3 Getting A Feel of the Dataset\\nLet’s display the ﬁrst few rows of the dataset to get a feel of it:\\n# Configuring float numbers format\\npd.options .display .float_format ='{:20.2f} '.format\\ndataset .head(n =5)\\n12Order PID MS SubClass MS Zoning Lot Frontage Lot Area\\n1 526301100 20 RL 141.00 31770\\n2 526350040 20 RH 80.00 11622\\n3 526351010 20 RL 81.00 14267\\n4 526353030 20 RL 93.00 11160\\n5 527105010 60 RL 74.00 13830\\nStreet Alley Lot Shape Land Contour Utilities Lot Conﬁg\\nPave NaN IR1 Lvl AllPub Corner\\nPave NaN Reg Lvl AllPub Inside\\nPave NaN IR1 Lvl AllPub Corner\\nPave NaN Reg Lvl AllPub Corner\\nPave NaN IR1 Lvl AllPub Inside\\nLand Slope Neighborhood Condition 1 Condition 2 Bldg Type House Style\\nGtl NAmes Norm Norm 1Fam 1Story\\nGtl NAmes Feedr Norm 1Fam 1Story\\nGtl NAmes Norm Norm 1Fam 1Story\",\n",
              " 'Pave NaN IR1 Lvl AllPub Inside\\nLand Slope Neighborhood Condition 1 Condition 2 Bldg Type House Style\\nGtl NAmes Norm Norm 1Fam 1Story\\nGtl NAmes Feedr Norm 1Fam 1Story\\nGtl NAmes Norm Norm 1Fam 1Story\\nGtl NAmes Norm Norm 1Fam 1Story\\nGtl Gilbert Norm Norm 1Fam 2Story\\nOverall Qual Overall Cond Year Built Year Remod/Add Roof Style Roof Matl\\n6 5 1960 1960 Hip CompShg\\n5 6 1961 1961 Gable CompShg\\n6 6 1958 1958 Hip CompShg\\n7 5 1968 1968 Hip CompShg\\n5 5 1997 1998 Gable CompShg\\nExterior 1st Exterior 2nd Mas Vnr Type Mas Vnr Area Exter Qual Exter Cond\\nBrkFace Plywood Stone 112.00 TA TA\\nVinylSd VinylSd None 0.00 TA TA\\nWd Sdng Wd Sdng BrkFace 108.00 TA TA\\nBrkFace BrkFace None 0.00 Gd TA\\nVinylSd VinylSd None 0.00 TA TA\\n13Foundation Bsmt Qual Bsmt Cond Bsmt Exposure BsmtFin Type 1 BsmtFin SF 1',\n",
              " 'Wd Sdng Wd Sdng BrkFace 108.00 TA TA\\nBrkFace BrkFace None 0.00 Gd TA\\nVinylSd VinylSd None 0.00 TA TA\\n13Foundation Bsmt Qual Bsmt Cond Bsmt Exposure BsmtFin Type 1 BsmtFin SF 1\\nCBlock TA Gd Gd BLQ 639.00\\nCBlock TA TA No Rec 468.00\\nCBlock TA TA No ALQ 923.00\\nCBlock TA TA No ALQ 1065.00\\nPConc Gd TA No GLQ 791.00\\nBsmtFin Type 2 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF Heating Heating QC\\nUnf 0.00 441.00 1080.00 GasA Fa\\nLwQ 144.00 270.00 882.00 GasA TA\\nUnf 0.00 406.00 1329.00 GasA TA\\nUnf 0.00 1045.00 2110.00 GasA Ex\\nUnf 0.00 137.00 928.00 GasA Gd\\nCentral Air Electrical 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area\\nY SBrkr 1656 0 0 1656\\nY SBrkr 896 0 0 896\\nY SBrkr 1329 0 0 1329\\nY SBrkr 2110 0 0 2110\\nY SBrkr 928 701 0 1629',\n",
              " 'Central Air Electrical 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area\\nY SBrkr 1656 0 0 1656\\nY SBrkr 896 0 0 896\\nY SBrkr 1329 0 0 1329\\nY SBrkr 2110 0 0 2110\\nY SBrkr 928 701 0 1629\\nBsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr\\n1.00 0.00 1 0 3 1\\n0.00 0.00 1 0 2 1\\n0.00 0.00 1 1 3 1\\n1.00 0.00 2 1 3 1\\n0.00 0.00 2 1 3 1\\nKitchen Qual TotRms AbvGrd Functional Fireplaces Fireplace Qu Garage Type\\nTA 7 Typ 2 Gd Attchd\\nTA 5 Typ 0 NaN Attchd\\nGd 6 Typ 0 NaN Attchd\\nEx 8 Typ 2 TA Attchd\\nTA 6 Typ 1 TA Attchd\\n14Garage Yr Blt Garage Finish Garage Cars Garage Area Garage Qual Garage Cond\\n1960.00 Fin 2.00 528.00 TA TA\\n1961.00 Unf 1.00 730.00 TA TA\\n1958.00 Unf 1.00 312.00 TA TA\\n1968.00 Fin 2.00 522.00 TA TA\\n1997.00 Fin 2.00 482.00 TA TA',\n",
              " '1960.00 Fin 2.00 528.00 TA TA\\n1961.00 Unf 1.00 730.00 TA TA\\n1958.00 Unf 1.00 312.00 TA TA\\n1968.00 Fin 2.00 522.00 TA TA\\n1997.00 Fin 2.00 482.00 TA TA\\nPaved Drive Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch\\nP 210 62 0 0 0\\nY 140 0 0 0 120\\nY 393 36 0 0 0\\nY 0 0 0 0 0\\nY 212 34 0 0 0\\nPool Area Pool QC Fence Misc Feature Misc Val Mo Sold\\n0 NaN NaN NaN 0 5\\n0 NaN MnPrv NaN 0 6\\n0 NaN NaN Gar2 12500 6\\n0 NaN NaN NaN 0 4\\n0 NaN MnPrv NaN 0 3\\nYr Sold Sale Type Sale Condition SalePrice\\n2010 WD Normal 215000\\n2010 WD Normal 105000\\n2010 WD Normal 172000\\n2010 WD Normal 244000\\n2010 WD Normal 189900\\nNow, let’s get statistical information about the numeric columns in our dataset. We want to\\nknow the mean, the standard deviation, the minimum, the maximum, and the 50th percentile (the',\n",
              " 'Now, let’s get statistical information about the numeric columns in our dataset. We want to\\nknow the mean, the standard deviation, the minimum, the maximum, and the 50th percentile (the\\nmedian) for each numeric column in the dataset:\\ndataset .describe(include =[np.number], percentiles =[.5]) \\\\\\n.transpose() .drop( \"count \", axis =1)\\n15mean std min 50% max\\nOrder 1465.50 845.96 1.00 1465.50 2930.00\\nPID 714464496.99 188730844.65 526301100.00 535453620.00 1007100110.00\\nMS SubClass 57.39 42.64 20.00 50.00 190.00\\nLot Frontage 69.22 23.37 21.00 68.00 313.00\\nLot Area 10147.92 7880.02 1300.00 9436.50 215245.00\\nOverall Qual 6.09 1.41 1.00 6.00 10.00\\nOverall Cond 5.56 1.11 1.00 5.00 9.00\\nYear Built 1971.36 30.25 1872.00 1973.00 2010.00\\nYear Remod/Add 1984.27 20.86 1950.00 1993.00 2010.00',\n",
              " 'Overall Qual 6.09 1.41 1.00 6.00 10.00\\nOverall Cond 5.56 1.11 1.00 5.00 9.00\\nYear Built 1971.36 30.25 1872.00 1973.00 2010.00\\nYear Remod/Add 1984.27 20.86 1950.00 1993.00 2010.00\\nMas Vnr Area 101.90 179.11 0.00 0.00 1600.00\\nBsmtFin SF 1 442.63 455.59 0.00 370.00 5644.00\\nBsmtFin SF 2 49.72 169.17 0.00 0.00 1526.00\\nBsmt Unf SF 559.26 439.49 0.00 466.00 2336.00\\nTotal Bsmt SF 1051.61 440.62 0.00 990.00 6110.00\\n1st Flr SF 1159.56 391.89 334.00 1084.00 5095.00\\n2nd Flr SF 335.46 428.40 0.00 0.00 2065.00\\nLow Qual Fin SF 4.68 46.31 0.00 0.00 1064.00\\nGr Liv Area 1499.69 505.51 334.00 1442.00 5642.00\\nBsmt Full Bath 0.43 0.52 0.00 0.00 3.00\\nBsmt Half Bath 0.06 0.25 0.00 0.00 2.00\\nFull Bath 1.57 0.55 0.00 2.00 4.00\\nHalf Bath 0.38 0.50 0.00 0.00 2.00\\nBedroom AbvGr 2.85 0.83 0.00 3.00 8.00',\n",
              " 'Bsmt Full Bath 0.43 0.52 0.00 0.00 3.00\\nBsmt Half Bath 0.06 0.25 0.00 0.00 2.00\\nFull Bath 1.57 0.55 0.00 2.00 4.00\\nHalf Bath 0.38 0.50 0.00 0.00 2.00\\nBedroom AbvGr 2.85 0.83 0.00 3.00 8.00\\nKitchen AbvGr 1.04 0.21 0.00 1.00 3.00\\nTotRms AbvGrd 6.44 1.57 2.00 6.00 15.00\\nFireplaces 0.60 0.65 0.00 1.00 4.00\\nGarage Yr Blt 1978.13 25.53 1895.00 1979.00 2207.00\\nGarage Cars 1.77 0.76 0.00 2.00 5.00\\nGarage Area 472.82 215.05 0.00 480.00 1488.00\\nWood Deck SF 93.75 126.36 0.00 0.00 1424.00\\nOpen Porch SF 47.53 67.48 0.00 27.00 742.00\\nEnclosed Porch 23.01 64.14 0.00 0.00 1012.00\\n3Ssn Porch 2.59 25.14 0.00 0.00 508.00\\nScreen Porch 16.00 56.09 0.00 0.00 576.00\\nPool Area 2.24 35.60 0.00 0.00 800.00\\nMisc Val 50.64 566.34 0.00 0.00 17000.00\\nMo Sold 6.22 2.71 1.00 6.00 12.00',\n",
              " '3Ssn Porch 2.59 25.14 0.00 0.00 508.00\\nScreen Porch 16.00 56.09 0.00 0.00 576.00\\nPool Area 2.24 35.60 0.00 0.00 800.00\\nMisc Val 50.64 566.34 0.00 0.00 17000.00\\nMo Sold 6.22 2.71 1.00 6.00 12.00\\nYr Sold 2007.79 1.32 2006.00 2008.00 2010.00\\nSalePrice 180796.06 79886.69 12789.00 160000.00 755000.00\\nFrom the table above, we can see, for example, that the average lot area of the houses in our\\ndataset is 10,147.92 ft2 with a standard deviation of 7,880.02 ft2. We can see also that the minimum\\nlot area is 1,300 ft2 and the maximum lot area is 215,245 ft2 with a median of 9,436.5 ft2. Similarly,\\nwe can get a lot of information about our dataset variables from the table.\\n16Then, we move to see statistical information about the non-numerical columns in our dataset:',\n",
              " 'we can get a lot of information about our dataset variables from the table.\\n16Then, we move to see statistical information about the non-numerical columns in our dataset:\\ndataset .describe(include =[np.object]) .transpose() \\\\\\n.drop( \"count \", axis =1)\\nunique top freq\\nMS Zoning 7 RL 2273\\nStreet 2 Pave 2918\\nAlley 2 Grvl 120\\nLot Shape 4 Reg 1859\\nLand Contour 4 Lvl 2633\\nUtilities 3 AllPub 2927\\nLot Conﬁg 5 Inside 2140\\nLand Slope 3 Gtl 2789\\nNeighborhood 28 NAmes 443\\nCondition 1 9 Norm 2522\\nCondition 2 8 Norm 2900\\nBldg Type 5 1Fam 2425\\nHouse Style 8 1Story 1481\\nRoof Style 6 Gable 2321\\nRoof Matl 8 CompShg 2887\\nExterior 1st 16 VinylSd 1026\\nExterior 2nd 17 VinylSd 1015\\nMas Vnr Type 5 None 1752\\nExter Qual 4 TA 1799\\nExter Cond 5 TA 2549\\nFoundation 6 PConc 1310\\nBsmt Qual 5 TA 1283\\nBsmt Cond 5 TA 2616',\n",
              " 'Exterior 1st 16 VinylSd 1026\\nExterior 2nd 17 VinylSd 1015\\nMas Vnr Type 5 None 1752\\nExter Qual 4 TA 1799\\nExter Cond 5 TA 2549\\nFoundation 6 PConc 1310\\nBsmt Qual 5 TA 1283\\nBsmt Cond 5 TA 2616\\nBsmt Exposure 4 No 1906\\nBsmtFin Type 1 6 GLQ 859\\nBsmtFin Type 2 6 Unf 2499\\nHeating 6 GasA 2885\\nHeating QC 5 Ex 1495\\nCentral Air 2 Y 2734\\nElectrical 5 SBrkr 2682\\nKitchen Qual 5 TA 1494\\nFunctional 8 Typ 2728\\nFireplace Qu 5 Gd 744\\nGarage Type 6 Attchd 1731\\nGarage Finish 3 Unf 1231\\nGarage Qual 5 TA 2615\\nGarage Cond 5 TA 2665\\nPaved Drive 3 Y 2652\\nPool QC 4 Gd 4\\nFence 4 MnPrv 330\\n17unique top freq\\nMisc Feature 5 Shed 95\\nSale Type 10 WD 2536\\nSale Condition 6 Normal 2413\\nIn the table we got, count represents the number of non-null values in each column, unique',\n",
              " 'Fence 4 MnPrv 330\\n17unique top freq\\nMisc Feature 5 Shed 95\\nSale Type 10 WD 2536\\nSale Condition 6 Normal 2413\\nIn the table we got, count represents the number of non-null values in each column, unique\\nrepresents the number of unique values, toprepresents the most frequent element, and freq rep-\\nresents the frequency of the most frequent element.\\n3.4 Data Cleaning\\n3.4.1 Dealing with Missing Values\\nWe should deal with the problem of missing values because some machine learning models don’t\\naccept data with missing values. Firstly, let’s see the number of missing values in our dataset.\\nWe want to see the number and the percentage of missing values for each column that actually\\ncontains missing values.\\n# Getting the number of missing values in each column\\nnum_missing =dataset .isna() .sum()']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "astra_vector_store.add_texts(texts[:50])\n",
        "print(\"inserted %i headlines.\" % len(texts[:50]))\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDDAbFWQHqWV",
        "outputId": "cec4a2d6-9471-4e16-c0c7-52d17c1bbdfa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inserted 50 headlines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_question = True\n",
        "while True:\n",
        "  if first_question:\n",
        "    query_text = input(\"\\nEnter your question(or type'quit' to exit):\").strip()\n",
        "  else:\n",
        "     query_text = input(\"\\nwhat your next question \").strip()\n",
        "  if query_text.lower() == \"quit\":\n",
        "      break\n",
        "  if query_text == \"\":\n",
        "    continue\n",
        "\n",
        "  first_question = False\n",
        "  print(\"\\n Question: \\\"%s\\\"\"%query_text)\n",
        "  answer = astra_vector_index.query(query_text,llm = llm).strip()\n",
        "  print(\"Answer: \\\"%s\\n\" % answer)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "JrxE2_EtM3Jw",
        "outputId": "7b9d57ef-60eb-4d26-91a6-9299553f5219"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter your question(or type'quit' to exit):what is dataset\n",
            "\n",
            " Question: \"what is dataset\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \"The dataset is a collection of data points or records that are used for analysis and machine learning models. It contains information about house sales in Ames, Iowa from 2006 to 2013 and includes features such as sale price, location, and neighborhood attributes. The dataset is divided into a training set and a test set for model training and testing purposes.\n",
            "\n",
            "\n",
            "what your next question what should i do regression model\n",
            "\n",
            " Question: \"what should i do regression model\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \"It is not clear what your specific question is, but based on the context provided, it appears that the study mentioned in the given context uses a Bayesian regularized neural network for stock price prediction. Other regression models that were used in similar studies include linear regression, nearest neighbors, support vector regression, decision tree, neural network, random forest, and gradient boosting. However, the best regression model for your specific problem would depend on the type of data and the specific goals of your analysis. It is best to consult with a data scientist or expert in regression modeling to determine the most appropriate approach for your project.\n",
            "\n",
            "\n",
            "what your next question tell me aboutclassification models\n",
            "\n",
            " Question: \"tell me aboutclassification models\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \"Classification models are a type of machine learning prediction technique used to categorize data into different groups or classes. These models use algorithms to analyze and classify data based on specific features or attributes. They are commonly used in various fields, such as finance, medicine, and marketing, to make predictions or decisions based on data. Some popular classification models include logistic regression, decision trees, and support vector machines.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-94bac4583bc4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mquery_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your question(or type'quit' to exit):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      \u001b[0mquery_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nwhat your next question \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3rUhPU1PBvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}